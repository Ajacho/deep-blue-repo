# Ux-10

## URL Submission and Analysis
### Story: 
> As a system, I want to be able to scrape static websites using htmlagilitypack so that I can analyze non-dynamic sites

### Assumptions/Preconditions
- The websites to be scraped do not require JavaScript rendering for data extraction.   
- The application has an existing .NET environment set up.    
- HtmlAgilityPack is a compatible and maintained library for parsing HTML in .NET.    
- The scraped data will be structured in a way that supports further analysis.    
- The system has network access to the target websites.

### Description
To enhance the systemâ€™s ability to analyze non-dynamic websites, we need to implement web scraping functionality using the HtmlAgilityPack library. This functionality will allow us to extract structured data such as headings, paragraphs, links, and metadata from static HTML pages. The extracted content will then be processed for further analysis, such as accessibility evaluation, or content validation.

The solution will involve integrating HtmlAgilityPack into an ASP.NET Core MVC application, ensuring that the scraping logic is encapsulated in a controller or service class. Unit tests will be written to validate the functionality, ensuring that different types of static web pages can be parsed correctly.

### Acceptance Criteria:
**Given** the system has HtmlAgilityPack installed and configured,   
**Then** I will be able to send a request to scrape a static webpage,   
**And** the system will successfully retrieve the HTML content of the page,   
**And** the content will be parsed into structured data without requiring JavaScript execution.   

**Given** a valid static website URL is provided for scraping,   
**Then** the system will extract the title, meta tags, headings (h1-h6) paragraphs, links, and images,   
**And** the extracted data will be formatted into a structured JSON object or database entry,     
**And** empty or malformed pages will return an appropriate error message without crashing the system.   

**Given** the system processes the scraped data,   
**Then** the parsed HTML elements will be stored or made available for further analysis,   
**And** data formatting will ensure proper separation of text, attributes, and links,    
**And** the system will be able to filter unnecessary elements    

**Given** a website fails to load or is inaccessible,   
**Then** the system will log an appropriate error message,   
**And** the failure will not disrupt other scraping requests,   
**And** the API will return a structured response indicating the failure reason   

### Tasks
- Install HtmlAgilityPack via NuGet.
- Write unit tests
- Write Jest tests
- Write Specflow tests
- Create a WebScraperService class to encapsulate the scraping logic.
- Develop a method to send HTTP requests and retrieve static HTML content.
- Use HtmlAgilityPack to load and parse the HTML document.
- Extract key elements
- Convert extracted data into structured JSON format.
- Store scraped content in a temporary in-memory collection for easy retrieval.
- Handle invalid URLs and HTTP failures
- Ensure the system gracefully handles empty or malformed HTML responses.
- Create an MVC Controller (ScraperController) to expose scraping functionality.
- Implement a GET method:
    - /scraper/extract?url={site_url}
    - Calls WebScraperService to fetch and return structured HTML data.
    - Returns a JSON response with extracted content or an error message if the request fails.


### Effort Points: 4
### Dependencies: UX-9
### Owner: 
### Branch: feature/static-scraping